seed:
  desc: Random seed
  value: 42
bert_class:
  desc: Class to be used on bert and on tokenizer
  value: "distilbert-base-uncased"
data_home:
  desc: Where to save data to
  value: "/ssd2/arthur/TRECCAR/data"
logging_level:
  desc: Level for Python logger
  value: "INFO"
force_steps:
  desc: Steps to forecefully run, even if the output file for the step is present
  value: t
number_of_cpus:
  desc: Number of CPUs to be used on parallel bits
  value: 24
split_percentage:
  desc: Percentage for test splitting
  value: .3
negative_samples:
  desc: Number of negative samples for each positive sample
  value: 10
ignore_gpu_ids:
  desc: GPU IDs to ignore when running
  value: [0, 1]
batch_per_device:
  desc: "Size of training batch for each GPU. Must be enough to fit in ONE GPU only.
  Final batch_size is num_gpus*this. Each sample, with max_len=512, takes about 522MB."
  value: 16
eval_batchsize:
  desc: Size of DEV batch. Make sure it fits in memory!
  value: 32
n_epochs:
  desc: Number of epochs to train BERT
  value: 2
learning_rate:
  desc: BERT learning rate
  value: 0.00005
gradient_accumulation_steps:
  desc: Steps to accumulate the gradient when training
  value: 1 
train_loss_print:
  desc: Steps to use before logging loss
  value: 50  # CHANGE THIS 
eval_steps: 
  desc: Steps to run before running an evaluate step
  value: 200  # CHANGE THIS
eval_sample:
    desc: Sample size for run on eval during training
    value: 1.0
