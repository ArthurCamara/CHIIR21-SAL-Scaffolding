seed:
  desc: Random seed
  value: 42
bert_class:
  desc: Class to be used on bert and on tokenizer
  value: "distilbert-base-uncased"
data_home:
  desc: Where to save data to
  value: "/ssd2/arthur/TRECCAR/data"
raw_data_home:
  desc: Path with TRECCAR raw data
  value: "/ssd2/arthur/TRECCAR"
logging_level:
  desc: Level for Python logger
  value: "INFO"
force_steps:
  desc: Steps to forecefully run, even if the output file for the step is present
  value: []
number_of_cpus:
  desc: Number of CPUs to be used on parallel bits
  value: 36
split_percentage:
  desc: Percentage for test splitting
  value: .3
negative_samples:
  desc: Number of negative samples for each positive sample
  value: 10
ignore_gpu_ids:
  desc: GPU IDs to ignore when running
  value: [0, 1]
batch_per_device:
  desc: "Size of training batch for each GPU. Must be enough to fit in ONE GPU only.
  Final batch_size is num_gpus*this. Each sample, with max_len=512, takes about 522MB."
  value: 16
eval_batchsize:
  desc: Size of DEV batch. Make sure it fits in memory!
  value: 32
n_epochs:
  desc: Number of epochs to train BERT
  value: 2
learning_rate:
  desc: BERT learning rate
  value: 0.00005
gradient_accumulation_steps:
  desc: Steps to accumulate the gradient when training
  value: 1 
train_loss_print:
  desc: Steps to use before logging loss
  value: 50  # CHANGE THIS 
eval_steps: 
  desc: Steps to run before running an evaluate step
  value: 200  # CHANGE THIS
eval_sample:
  desc: Sample size for run on eval during training
  value: 1.0
test_topics:
  desc: Number of topics in the test dataset (Absolute, not only leaf)
  value: 722
corpus_size:
  desc: Number of documents
  value: 29794697

indri_bin_path:
  desc: Path with indri binaries
  value: /ssd2/arthur/indri/bin
indri_top_k:
  desc: Number of docs to retrieve from Indro
  value: 100
index_path:
  desc: Path with Indri index with all wikipedia articles
  value: /ssd2/arthur/TRECCAR/indexes/wikipedia_no_extra
possible_relevants:
  desc: number of documents to be considered when building bert test file
  value: 500
bert_alpha:
  desc: Alpha value to cobining BERT score with QL score -> alpha*BERT + (1-alpha) * QL
  value: 0.85
relevance_threshold:
  desc: Minimum value to consider a document relevant to the unit or subtopic
  value: 0.87
cutoff_percentile:
  desc: Percentile for relevance cutoff
  value: 80
topics:
  desc: Topics to be used in the experiments
  value: ["Subprime mortgage crisis", "Theory of mind", "Research in lithium-ion batteries", "Ethics", "Business cycle", "Genetically modified organism", "Norepinephrine", "Irritable bowel syndrome", "Radiocarbon dating considerations", "Noise-induced hearing loss"]
    
    