{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the following data points:\n",
    "- Outlines titles for each topic\n",
    "    - train (✅)\n",
    "    - test (✅)\n",
    "- Wikipedia articles (paragraphs) (✅) - 29794697 documents\n",
    "- Relevant matching between outlines on TQA and wikipedia article\n",
    "    - train - `/ssd2/arthur/TRECCAR/benchmarkY1/benchmarkY1-train/train.pages.cbor-hierarchical.qrels` (✅)\n",
    "\n",
    "`trec-car-tools` documentation: https://trec-car-tools.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T10:40:19.207388Z",
     "start_time": "2019-11-26T10:40:19.186137Z"
    }
   },
   "outputs": [],
   "source": [
    "data_home = \"/ssd2/arthur/TRECCAR/\"\n",
    "import os\n",
    "from trec_car import read_data\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:17:32.368895Z",
     "start_time": "2019-11-27T15:04:41.060306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51494a562df04a7f9da37d1e48c5d347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs_path = os.path.join(data_home, \"paragraphCorpus/dedup.articles-paragraphs.cbor\")\n",
    "doc_format = \"<DOC>\\n<DOCNO>{}</DOCNO>\\n<TEXT>{}</TEXT>\\n</DOC>\\n\"\n",
    "corpus_path = os.path.join(data_home, \"data\")\n",
    "if not os.path.isdir(corpus_path):\n",
    "    os.mkdir(corpus_path)\n",
    "corpus_path = \"../../TRECCAR/data/wikipedia_paragaphs.trec\"\n",
    "all_docs = dict()\n",
    "with open(corpus_path, 'w', encoding=\"utf-8\") as outf:\n",
    "    for paragraph in tqdm(read_data.iter_paragraphs(open(paragraphs_path, 'rb'))):\n",
    "        text = paragraph.get_text()\n",
    "        paragraph_id = paragraph.para_id\n",
    "        outf.write(doc_format.format(paragraph_id, text))\n",
    "        all_docs[paragraph_id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:51:19.028042Z",
     "start_time": "2019-11-27T14:51:19.015511Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_page(headings, depth=1):\n",
    "    for i in headings:\n",
    "        print(\"\\t\"*depth, i[0].heading)\n",
    "        print_page(i[1], depth=depth+1)\n",
    "\n",
    "def unrol_headings(headings, parents, hierarchy=[]):\n",
    "    all_headings = []\n",
    "    for i in headings:\n",
    "        cannonical_name = parents+\"/\"+i[0].headingId\n",
    "        all_headings.append((cannonical_name, i[0].heading, hierarchy + [i[0].heading]))\n",
    "        all_headings += (unrol_headings(i[1], cannonical_name, hierarchy + [i[0].heading]))\n",
    "    return all_headings\n",
    "a = unrol_headings(page.deep_headings_list(), page.page_id, hierarchy = [page.page_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:00:40.313813Z",
     "start_time": "2019-11-27T15:00:40.082224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843343c8854d40a1bc20091fa9bb7931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff27ddf8614d75a21fdd1752c29fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topics_test = \"../../TRECCAR/benchmarkY2.public/benchmarkY2.cbor-outlines.cbor\"\n",
    "multi_level = 0\n",
    "test_topics = []\n",
    "train_topics = []\n",
    "nested_topics = dict()\n",
    "test_topics_path = \"../../TRECCAR/data/topics/test-topics_flat.txt\"\n",
    "with open(test_topics_path, 'w', encoding='utf-8') as outf:\n",
    "    for page in tqdm(read_data.iter_annotations(open(topics_test, 'rb'))):\n",
    "        level_1_headings = len(page.nested_headings())\n",
    "        l = page.flat_headings_list()\n",
    "        flat_list = unrol_headings(page.deep_headings_list(), page.page_id, hierarchy = [page.page_name])\n",
    "        if len(flat_list) != level_1_headings:\n",
    "            multi_level +=1\n",
    "            test_topics+=flat_list\n",
    "            for _id, name, hierarchy in flat_list:\n",
    "                outf.write(\"{};{};{}\\n\".format(_id, name, \" \".join(hierarchy)))\n",
    "            \n",
    "        \n",
    "\n",
    "train_path = \"../../TRECCAR/benchmarkY1/benchmarkY1-train/train.pages.cbor-outlines.cbor\"\n",
    "train_topics_path = \"../../TRECCAR/data/topics/train-topics_flat.txt\"\n",
    "with open(train_topics_path, 'w', encoding='utf-8') as outf:\n",
    "    for page in tqdm(read_data.iter_annotations(open(train_path, 'rb'))):\n",
    "        level_1_headings = len(page.nested_headings())\n",
    "        l = page.flat_headings_list()\n",
    "        flat_list = unrol_headings(page.deep_headings_list(), page.page_id, hierarchy = [page.page_name])\n",
    "        train_topics+=flat_list\n",
    "        for _id, name, hierarchy in flat_list:\n",
    "            outf.write(\"{};{};{}\\n\".format(_id, name, \" \".join(hierarchy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:47:40.430587Z",
     "start_time": "2019-11-27T14:47:39.938933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create BERT training file\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:03:31.446653Z",
     "start_time": "2019-11-27T15:03:31.387187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c8a9b22e36447fae822f1fca3900e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load relevants\n",
    "from collections import defaultdict\n",
    "relevants = defaultdict(lambda: set())\n",
    "all_docs = \n",
    "for line in tqdm(open('/ssd2/arthur/TRECCAR/benchmarkY1/benchmarkY1-train/train.pages.cbor-hierarchical.qrels')):\n",
    "    topic, _, doc, label = line.strip().split()\n",
    "    if label == \"1\":\n",
    "        relevants[topic].add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:20:37.864866Z",
     "start_time": "2019-11-27T15:20:37.858724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a101e6a01b2bb9af9ab43345cc8c26737a1bcec9'}"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevants[canonical_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:34:54.309831Z",
     "start_time": "2019-11-27T15:34:53.200622Z"
    }
   },
   "outputs": [],
   "source": [
    "all_docs_ids = list(all_docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:31:25.559182Z",
     "start_time": "2019-11-27T15:31:25.555155Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:35:01.209285Z",
     "start_time": "2019-11-27T15:35:01.202468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b026b68d99ce6d52f79a902aa8dc7db79c2fdba4'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(all_docs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:35:53.022285Z",
     "start_time": "2019-11-27T15:35:53.016320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1977"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:36:11.346861Z",
     "start_time": "2019-11-27T15:36:00.239825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f6c6190006493b98908de8f5b2554d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1977), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../TRECCAR/data/samples/triples_tokenized.bert\", 'w', encoding='utf-8') as outf:\n",
    "    for counter, (canonical_id, topic, hierarchy) in tqdm(enumerate(train_topics), total = len(train_topics)):\n",
    "        query = \" \".join(hierarchy)\n",
    "        tokenized_query = tokenizer.tokenize(query)\n",
    "        relevant_docs = relevants[canonical_id]\n",
    "        for doc in relevants[canonical_id]:\n",
    "            triple_id = \"{}-{}\".format(canonical_id, doc)\n",
    "            doc_text = all_docs[doc]\n",
    "            tokenized_doc = tokenizer.tokenize(doc_text)\n",
    "            final_version = ['[CLS]']+tokenized_query+['[SEP]']+tokenized_doc+['[SEP]']\n",
    "            outf.write(\"{}\\t{}\\t1\\n\".format(triple_id, str(final_version)))\n",
    "            #negative sampling\n",
    "            neg_doc = random.choice(all_docs_ids)\n",
    "            doc_text = all_docs[neg_doc]\n",
    "            tokenized_doc = tokenizer.tokenize(doc_text)\n",
    "            final_version = ['[CLS]']+tokenized_query+['[SEP]']+tokenized_doc+['[SEP]']\n",
    "            outf.write(\"{}\\t{}\\t0\\n\".format(triple_id, str(final_version)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
