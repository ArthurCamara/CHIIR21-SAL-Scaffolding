wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.8.16
    framework: torch
    is_jupyter_run: true
    python_version: 3.7.3
batch_per_device:
  desc: Size of training batch for each GPU. Must be enough to fit in ONE GPU only.
    Final batch_size is num_gpus*this. Each sample, with max_len=512, takes about
    522MB.
  value: 16
bert_class:
  desc: Class to be used on bert and on tokenizer
  value: distilbert-base-uncased
data_home:
  desc: Where to save data to
  value: /ssd2/arthur/TRECCAR/data
eval_batchsize:
  desc: Size of DEV batch. Make sure it fits in memory!
  value: 32
eval_sample:
  desc: Sample size for run on eval during training
  value: 1.0
eval_steps:
  desc: Steps to run before running an evaluate step
  value: 40
force_steps:
  desc: Steps to forecefully run, even if the output file for the step is present
  value: t
gradient_accumulation_steps:
  desc: Steps to accumulate the gradient when training
  value: 1
ignore_gpu_ids:
  desc: GPU IDs to ignore when running
  value:
  - 0
  - 1
learning_rate:
  desc: BERT learning rate
  value: 5.0e-05
logging_level:
  desc: Level for Python logger
  value: INFO
max_document_len:
  desc: Maximum number of tokens to be considered in the document for the cut version
    of the dataset
  value: 500
n_epochs:
  desc: Number of epochs to train BERT
  value: 2
negative_samples:
  desc: Number of negative samples for each positive sample
  value: 1
number_of_cpus:
  desc: Number of CPUs to be used on parallel bits
  value: 24
seed:
  desc: Random seed
  value: 42
split_percentage:
  desc: Percentage for test splitting
  value: 0.3
train_loss_print:
  desc: Steps to use before logging loss
  value: 20
